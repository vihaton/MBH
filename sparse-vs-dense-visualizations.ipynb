{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.utils import *\n",
    "from scripts.models import *\n",
    "from scripts.visualizations import *\n",
    "from scripts.pruning import *\n",
    "from scripts.activation_patterns import *\n",
    "from scripts.stats import *\n",
    "from scripts.activation_regions import *\n",
    "from scripts.lp_decision_trees import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams\n",
    "...That you actually change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = 'visualizations'\n",
    "test_name = 'example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figures = False\n",
    "\n",
    "results_from_training = False\n",
    "visualize_AP_based = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select test to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_root_dir = './results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folders = get_folders(results_root_dir)\n",
    "results_folders.sort()\n",
    "for i, folder in enumerate(results_folders):\n",
    "    print(i, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which results should we visualize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = results_root_dir + results_folders[result_ind] + '/'\n",
    "files = get_files_in_dir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = config = hyperparams = metadata = None\n",
    "for file in files:\n",
    "    name, end = file.split('.')\n",
    "    \n",
    "    print('read', name, end)\n",
    "    \n",
    "    if name == 'data':\n",
    "        data = torch.load(results_dir + file)\n",
    "    elif name == 'hyperparams':\n",
    "        hyperparams = torch.load(results_dir + file)\n",
    "    elif name == 'config':\n",
    "        with open(results_dir + file, 'rb') as fo:\n",
    "            if end == 'json':\n",
    "                config = json.loads(fo.read())\n",
    "            else:\n",
    "                config_b = fo.read()\n",
    "    elif name == 'metadata':\n",
    "        with open(results_dir + file, 'rb') as fo:\n",
    "            if end == 'json':\n",
    "                metadata = json.loads(fo.read())\n",
    "            else:\n",
    "                metadata_b = fo.read()\n",
    "    else:\n",
    "        print('unexpected file ', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparams is None:\n",
    "    \n",
    "    def try_fetch_hyperparams(model_root):\n",
    "        model_dir = model_root + results_folders[result_ind] + '/'\n",
    "        print('try to fetch hyperparams from', model_dir)\n",
    "        if not os.path.isdir(model_dir):\n",
    "            print(f'\\tdir {model_dir} didnt exist!')\n",
    "            return None\n",
    "        \n",
    "        files = get_files_in_dir(model_dir)\n",
    "        for file in files:\n",
    "            if 'hyperparams' in file:\n",
    "                hyperparams = torch.load(model_dir + file)\n",
    "                print('Success!')\n",
    "                \n",
    "        return hyperparams\n",
    "        \n",
    "    model_root = '../mnist/models/'\n",
    "    model_root = './models/'\n",
    "    \n",
    "    hyperparams = try_fetch_hyperparams(model_root)\n",
    "    \n",
    "print(hyperparams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "    print(key, data[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ar_2d'].keys() if not results_from_training else data['ar_2d_training'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    print(config.keys())\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_metada_from_hyperparams(hyperparams):\n",
    "    param_keys = OrderedDict()\n",
    "    for name in hyperparams:\n",
    "        param_keys[name] = [\n",
    "            hyp[0][-1] for hyp in hyperparams[name]\n",
    "        ]\n",
    "\n",
    "    model_architectures = []\n",
    "    for name in hyperparams:\n",
    "        model_architectures.append(name)\n",
    "    return param_keys, model_architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_training_setup = config['data'].get('use_training_setup', True) if not results_from_training else True\n",
    "\n",
    "data_conf_str = 'training metadata' if use_training_setup else 'configuration file'\n",
    "print(f'get data configuration from {data_conf_str}')\n",
    "\n",
    "data_conf = config['data'] if not use_training_setup else metadata['data']\n",
    "\n",
    "normalize_data = data_conf[\"normalize_data\"]\n",
    "normalize_by_features = data_conf[\"normalize_by_features\"]\n",
    "\n",
    "normalize_by_moving = data_conf[\"normalize_by_moving\"]\n",
    "normalize_by_scaling = data_conf[\"normalize_by_scaling\"]\n",
    "\n",
    "batch_size = data_conf[\"batch_size\"]\n",
    "if not use_training_setup:\n",
    "    test_batch_size = data_conf[\"test_batch_size\"] if not 'test_batch_size' in config['data'] else config['data']['test_batch_size']\n",
    "else:\n",
    "    test_batch_size = data_conf['test_batch_size']\n",
    "\n",
    "# digit, fashion\n",
    "data_name = data_conf[\"data_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = data_conf[\"data_dir\"]\n",
    "print(data_name, 'from', training_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = metadata['models']\n",
    "\n",
    "sizes = model_conf.get('sizes', None)\n",
    "n_sizes = len(sizes) if sizes else 0\n",
    "\n",
    "iterations = model_conf.get('iterations', None)\n",
    "\n",
    "bias_std = model_conf.get('bias_std', None)\n",
    "\n",
    "hidden_dims_dict = model_conf.get('hidden_dims_dict', None)\n",
    "\n",
    "n_models = model_conf.get('n_models', None)\n",
    "\n",
    "models_total = model_conf.get('models_total', None)\n",
    "\n",
    "smallest_net_densities = model_conf.get('smallest_net_densities', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keys, model_architectures = infer_metada_from_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sizes, iterations, bias_std, hidden_dims_dict, n_models, models_total, smallest_net_densities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = metadata['models']\n",
    "\n",
    "evaluation_scheme = train_config.get('evaluation_scheme', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_scheme, len(evaluation_scheme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_conf = metadata['pruning']\n",
    "\n",
    "prune_weights = pruning_conf['prune_weights'] # False -> prune nodes\n",
    "layer_wise_pruning = pruning_conf['layer_wise_pruning'] # False -> global pruning (not implemented for pytorch pruning)\n",
    "\n",
    "prune_all_layers = pruning_conf['prune_all_layers'] # should also the weights on the output layer be pruned?\n",
    "\n",
    "random_mask = pruning_conf['random_mask'] # does the random init network have a random mask as well?\n",
    "\n",
    "xscale = 'linear' # depends how we define the pruning rates, either linear or logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prune_weights, layer_wise_pruning, prune_all_layers, random_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_from_training:\n",
    "    stat_conf = metadata['experiments']\n",
    "else:\n",
    "    stat_conf = config['experiments']\n",
    "stat_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting ARs\n",
    "compute_stats = stat_conf.get('compute_stats', True)\n",
    "\n",
    "## 2D\n",
    "classes = stat_conf.get('classes', None)\n",
    "\n",
    "# how much one class samples should be covered?\n",
    "class_coverage = stat_conf.get('class_coverage', None)\n",
    "\n",
    "use_three_samples = not stat_conf.get('plane_through_origin', True)\n",
    "\n",
    "average_over_images = stat_conf.get('average_over_images', None)\n",
    "\n",
    "# Mining LPs with Decision Trees\n",
    "lp_samples = stat_conf.get('lp_samples', None)\n",
    "\n",
    "\n",
    "arc_layers = {\n",
    "    'lenet': 1,\n",
    "    'deepfc': 2,\n",
    "    'pipefc': 2\n",
    "}\n",
    "\n",
    "'''\n",
    "# Dark neurons\n",
    "dm_limit_perc = 0.01\n",
    "dark_mask_limit = int(dm_limit_perc*test_batch_size)\n",
    "print(f'dark mask flags neurons with n <= {dark_mask_limit} images (out of {test_batch_size})')\n",
    "'''\n",
    "\n",
    "print(compute_stats, classes, class_coverage, use_three_samples, average_over_images, lp_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = f'visualizations-{ts}-{test_name}/'\n",
    "saving_folder = results_dir + folder_name\n",
    "if (save_figures) and not os.path.isdir(saving_folder):\n",
    "    os.mkdir(saving_folder)\n",
    "    print('created folder for saving figures')\n",
    "\n",
    "if save_figures:\n",
    "    print(f'save images & data to {saving_folder}')\n",
    "else:\n",
    "    print(\"don't save figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list(hyperparams.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - hyperparams[name][-1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'each set of hyperparams has {n_models} models')\n",
    "print()\n",
    "print_hyperparams(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Accuracy  \n",
    "2) Activation Pattern based\n",
    "    - Unique activation patterns\n",
    "    - Unique layer patterns\n",
    "    - Entropy\n",
    "    - Purity\n",
    "3) Changes in weights  \n",
    "4) Number of Activation Regions (Boris & David)  \n",
    "5) Weight distributions  \n",
    "6) #AR by iteration\n",
    "    - For the network as a whole\n",
    "    - By layer\n",
    "7) Dark Neurons  \n",
    "    - How many horizons the datapoints were over?  \n",
    "    - Distribution of Horizons  \n",
    "    - Dark neuron movements  \n",
    "10) AR distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = data['accuracy'] if not results_from_training else data['acc_training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    sparse_from = model_architectures[0]\n",
    "    # for prese\n",
    "    plt.figure(figsize=(8,6), dpi=250).patch.set_facecolor('w')\n",
    "    accs = np.array(acc_data['trained'][sparse_from][-2])\n",
    "    densities = parse_densities_from_list_of_hyperparams(hyperparams[sparse_from][-2])\n",
    "\n",
    "    plt.plot(densities, accs[:,0], marker='o', markerfacecolor='k', c='b')\n",
    "    plt.fill_between(densities, accs[:,1], accs[:,2], color='b', alpha=0.1)\n",
    "\n",
    "    plt.xlabel('density')\n",
    "    plt.ylabel('val accuracy (%)')\n",
    "\n",
    "    plt.title('Validation accuracy of 4 networks with 9.5k parameters each')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the smallest models to largest models, and most sparse to most dense (large) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    print('--- SD to LS ---')\n",
    "    small_param_acc = np.array(acc_data['trained'][name][-1])[:,0]\n",
    "    print(small_param_acc)\n",
    "    print(round((small_param_acc[0] - small_param_acc[-1])/100, 4))\n",
    "    \n",
    "    print('--- LS to LD ---')\n",
    "    large_param_acc = np.array(acc_data['trained'][name][0])[:,0]\n",
    "    print(large_param_acc)\n",
    "    print(round((large_param_acc[0] - small_param_acc[0])/100, 4))\n",
    "    \n",
    "    print('--- SD to LD ---')\n",
    "    print(round((large_param_acc[0] - small_param_acc[-1])/100, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    for scenario in acc_data:\n",
    "        for name in acc_data[scenario]:\n",
    "\n",
    "            def visualize_acc_hyperp(over_neurons, ylim, auto_ylim):\n",
    "                onstr = 'over-neurons' if over_neurons else 'over-densities'\n",
    "                ystr = f'ylim_{ylim}' if auto_ylim else 'ylim_auto'\n",
    "                scstr = scenario if not 'trained' in scenario else f'{scenario} {iterations} iterations'\n",
    "\n",
    "                draw_validation_accuracy(acc_data[scenario],\n",
    "                                         name,\n",
    "                                         scstr,\n",
    "                                         hyperparams,\n",
    "                                         over_neurons,\n",
    "                                         filename=saving_folder +\n",
    "                                         f'accuracy_{scenario}_{name}_{onstr}_{ystr}.png',\n",
    "                                         ylim=ylim,\n",
    "                                         auto_ylim=auto_ylim,\n",
    "                                         save_figures=save_figures)\n",
    "\n",
    "            over_n = [False, True]\n",
    "            if data_name == 'fashion':\n",
    "                ylims = [(50,100), (82,94)] if scenario == 'trained' and name == 'lenet' else [(0,100)]\n",
    "                ylims = [(50,100), (72,94)] if scenario == 'trained' and name == 'deepfc' else ylims\n",
    "                auto_ylims = [False, True] if scenario == 'trained' else [False]\n",
    "            else:\n",
    "                ylims = [(50,100), (90,100)] if scenario == 'trained' else [(0,100)]\n",
    "                auto_ylims = [False, True] if scenario == 'trained' else [False]\n",
    "\n",
    "\n",
    "            for over_neurons in over_n:\n",
    "                for i, ylim in enumerate(ylims):\n",
    "                    visualize_acc_hyperp(over_neurons, ylim, auto_ylims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if results_from_training:\n",
    "    sparse_from='lenet'\n",
    "    xscale = 'linear'\n",
    "    filename = saving_folder + f'accuracy-{sparse_from}_' + 'p{}' + f'_{xscale}.png'\n",
    "    for sparse_from in acc_data:\n",
    "        draw_validation_accuracy_from_training(acc_data, sparse_from, evaluation_scheme,\n",
    "                                               hyperparams_for_sparse=hyperparams,\n",
    "                                               n_models=n_models,\n",
    "                                               xscale=xscale,\n",
    "                                              filename=filename,\n",
    "                                              save_figures=save_figures)\n",
    "\n",
    "    filename = saving_folder + f'accuracy-{sparse_from}_' + 'p{}' + f'_{xscale}_xlim100.png'\n",
    "    for sparse_from in acc_data:\n",
    "        draw_validation_accuracy_from_training(acc_data, sparse_from,\n",
    "                                               evaluation_scheme,\n",
    "                                               hyperparams_for_sparse=hyperparams,\n",
    "                                               n_models=n_models,\n",
    "                                               xscale=xscale,\n",
    "                                               xlim=(-1,100),\n",
    "                                              filename=filename,\n",
    "                                              save_figures=save_figures)\n",
    "\n",
    "    xscale = 'log'\n",
    "    filename = saving_folder + f'accuracy-{sparse_from}_' + 'p{}' + f'_{xscale}.png'\n",
    "    for sparse_from in acc_data:\n",
    "        draw_validation_accuracy_from_training(acc_data, sparse_from, evaluation_scheme,\n",
    "                                               hyperparams_for_sparse=hyperparams,\n",
    "                                               n_models=n_models,\n",
    "                                               xscale=xscale,\n",
    "                                              filename=filename,\n",
    "                                              save_figures=save_figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_hyperparam_setup(hyperparams, filename=saving_folder + 'hyperparameter_setup.png',\n",
    "                      save_figures=save_figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    specialization_data = data['specialization']\n",
    "    print(specialization_data.keys())\n",
    "elif compute_stats:\n",
    "    # now the specialization data has shape\n",
    "    # architecture: params: [#hyperp, #models, #evaluations, #subspaces, #coverages, #metrics]\n",
    "    #   e.g. lenet: 120000: (1, 3, 28, 5, 4, 23)\n",
    "    specialization_data = data['spec_training']\n",
    "else:\n",
    "    specialization_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if specialization_data is not None:\n",
    "    pre_aggregated = specialization_data['trained'][name][0].shape[-1] == 3\n",
    "    print('data is pre-aggregated ', pre_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    large_param_spec = np.mean(specialization_data['trained'][name][0][:,:,0,1,0])\n",
    "    small_param_spec = np.mean(specialization_data['trained'][name][-1][:,:,0,1,0], axis=1)\n",
    "    print(large_param_spec.shape, small_param_spec.shape)\n",
    "    print(round(large_param_spec*100, 2), small_param_spec*100)\n",
    "    print('SD to LS', round(small_param_spec[0] - small_param_spec[-1],4))\n",
    "    print('SD to LD', round(large_param_spec - small_param_spec[-1],4))\n",
    "    print('LS to LD', round(large_param_spec - small_param_spec[0],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "\n",
    "    sparse_from = 'lenet'\n",
    "\n",
    "    metrics_s = [\n",
    "        'specialization ratio',\n",
    "        'total area classes',\n",
    "        'local area'\n",
    "    ]\n",
    "\n",
    "    metrics_ac = [f'area of class {i}' for i in range(10)]\n",
    "    \n",
    "    metrics_sc = [f'specialization of class {i}' for i in range(10)]\n",
    "\n",
    "    metrics_spec_all = metrics_s + metrics_ac\n",
    "    \n",
    "    if not pre_aggregated:\n",
    "        metrics_spec_all += metrics_sc\n",
    "\n",
    "    metrics_spec_dict = {\n",
    "        m: i for i, m in enumerate(metrics_spec_all)\n",
    "    }\n",
    "\n",
    "    print(metrics_spec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_filename_end(sparse_from, wide_interval, draw_init, over_neurons, class_cov_i):\n",
    "    distr = '_with-init' if draw_init else ''\n",
    "    wistr = 'wide' if wide_interval else 'narrow'\n",
    "    orstr = 'over-neurons' if over_neurons else 'over-density'\n",
    "    ccstr = f'_cc{round(class_coverage[class_cov_i]*1000)}' if class_cov_i else ''\n",
    "    return f'{sparse_from}_{orstr}_{wistr}{distr}{ccstr}.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(class_coverage) == list:\n",
    "    class_cov_i = 1\n",
    "    print(f'Use class coverage {class_coverage[class_cov_i]} as the default')\n",
    "else:\n",
    "    print(f'class coverage {class_coverage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    \n",
    "    draw_method = draw_aggregated_specialization_data if pre_aggregated else draw_specialization_data\n",
    "\n",
    "    for name in specialization_data[list(specialization_data.keys())[0]]:\n",
    "\n",
    "        def visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i=None):\n",
    "            file_stub = f'specialization_data_{plot_name}_{get_spec_filename_end(name, wide_interval, draw_init, over_neurons, class_cov_i)}'\n",
    "            draw_method(specialization_data,\n",
    "                        metrics,\n",
    "                        name,\n",
    "                        over_neurons,\n",
    "                        param_keys,\n",
    "                        lp_samples,\n",
    "                        hyperparams,\n",
    "                        wide_interval,\n",
    "                        arc_layers,\n",
    "                        metrics_spec_dict,\n",
    "                        average_over_images,\n",
    "                        use_three_samples,\n",
    "                        class_cov_i=class_cov_i,\n",
    "                        class_coverages=class_coverage,\n",
    "                        draw_init=draw_init,\n",
    "                        xscale=xscale,\n",
    "                        filename=saving_folder + file_stub,\n",
    "                        save_figures=save_figures)\n",
    "\n",
    "        if type(class_coverage) == list:\n",
    "            \n",
    "            # single specialization figures over density\n",
    "            for cc_i in range(len(class_coverage)):\n",
    "                plot_name = '1_specialmeasure'\n",
    "                metrics = [metrics_s[0]] # specialization\n",
    "                wide_interval = False\n",
    "                draw_init = False\n",
    "                over_neurons = False\n",
    "                visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, cc_i)\n",
    "                \n",
    "            # single specialization figures over neurons\n",
    "            for cc_i in range(len(class_coverage)):\n",
    "                plot_name = '1_specialmeasure_overn'\n",
    "                over_neurons = True\n",
    "                visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, cc_i)\n",
    "\n",
    "        else:\n",
    "            plot_name = '1_specialmeasure'\n",
    "            metrics = [metrics_s[0]]\n",
    "            wide_interval = False\n",
    "            draw_init = False\n",
    "            over_neurons = False\n",
    "            visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "            \n",
    "            plot_name = '1_specialmeasure_overn'\n",
    "            over_neurons = True\n",
    "            visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "        plot_name = ''\n",
    "        metrics = metrics_s\n",
    "        wide_interval = False\n",
    "        draw_init = False\n",
    "        over_neurons = False\n",
    "        visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "\n",
    "        wide_interval = True\n",
    "        draw_init = False\n",
    "        over_neurons = False\n",
    "        visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "\n",
    "        wide_interval = False\n",
    "        draw_init = True\n",
    "        over_neurons = False\n",
    "        visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "\n",
    "        wide_interval = False\n",
    "        draw_init = True\n",
    "        over_neurons = True\n",
    "        visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "\n",
    "        plot_name = 'spec_classes' if not pre_aggregated else 'area_classes'\n",
    "        metrics = metrics_sc if not pre_aggregated else metrics_ac\n",
    "        wide_interval = False\n",
    "        draw_init = False\n",
    "        over_neurons = False\n",
    "        visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n",
    "\n",
    "        if 'init' in specialization_data.keys():\n",
    "            wide_interval = False\n",
    "            draw_init = True\n",
    "            over_neurons = False\n",
    "            visualize_specialization_hyperp(plot_name, metrics, name, wide_interval, draw_init, over_neurons, class_cov_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy as the function of specialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenario = 'trained'\n",
    "sparse_from = name\n",
    "\n",
    "if not results_from_training or compute_stats:\n",
    "\n",
    "    for cov_i in list(range(len(class_coverage))):\n",
    "        aggregated_spec = specialization_data[scenario][sparse_from][0].shape[-1] == 3 # if the data is already aggregated, the last values are ave,min,max.\n",
    "\n",
    "        filename = saving_folder + 'accuracy_over_spec_' + scenario + '_cov_' + str(round(class_coverage[cov_i]*1000)) + '.png'\n",
    "\n",
    "        draw_validation_accuracy_over_specialization(acc_data[scenario], \n",
    "                                                     specialization_data,\n",
    "                                                     cov_i=cov_i,\n",
    "                                                     coverages=class_coverage,\n",
    "                                                     sparse_from=sparse_from,\n",
    "                                                     scenario=scenario,\n",
    "                                                     hyperparams=hyperparams,\n",
    "                                                     auto_ylim=True,\n",
    "                                                     aggregated_spec=aggregated_spec,\n",
    "                                                     filename=filename,\n",
    "                                                     save_figures=save_figures,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specialization as a function of minimum coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_spec_over_cc(scenario, draw_init=False):\n",
    "    instr = '_with_init' if draw_init and 'init' not in scenario else ''\n",
    "    filename = saving_folder + 'specialization_over_min_cov_p{}_' + scenario + instr + '.png'\n",
    "    return filename\n",
    "\n",
    "\n",
    "if not results_from_training:\n",
    "    draw_method = draw_specialization_data_over_min_coverage if not pre_aggregated else draw_aggregated_specialization_data_over_min_coverage\n",
    "    draw_init = False\n",
    "\n",
    "    if 'init' in list(specialization_data.keys()):\n",
    "        scenario = 'init'\n",
    "        linestyles = ['--']\n",
    "        filename = get_filename_spec_over_cc(scenario, draw_init)\n",
    "\n",
    "        draw_method(specialization_data, \n",
    "                    name, \n",
    "                    class_coverage, \n",
    "                    param_keys, \n",
    "                    lp_samples, \n",
    "                    hyperparams=hyperparams,\n",
    "                    draw_init=draw_init,\n",
    "                    average_over_images=average_over_images, \n",
    "                    use_three_samples=use_three_samples, \n",
    "                    linestyles=linestyles,\n",
    "                    scenario=scenario,\n",
    "                    filename=filename,\n",
    "                    save_figures=save_figures)\n",
    "    \n",
    "    scenario = 'trained'\n",
    "    linestyles = ['-']\n",
    "    filename = get_filename_spec_over_cc(scenario, draw_init)\n",
    "\n",
    "    draw_method(specialization_data, \n",
    "                name, \n",
    "                class_coverage, \n",
    "                param_keys, \n",
    "                lp_samples, \n",
    "                hyperparams=hyperparams,\n",
    "                draw_init=draw_init,\n",
    "                average_over_images=average_over_images, \n",
    "                use_three_samples=use_three_samples, \n",
    "                linestyles=linestyles,\n",
    "                scenario=scenario,\n",
    "                filename=filename,\n",
    "                save_figures=save_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_spec_over_iter(cov_i, xscale, xlim=None):\n",
    "    limstr = f'_xlim{xlim[1]}' if xlim else ''\n",
    "    cov_str = str([class_coverage[i] for i in cov_i]).replace(' ', '')\n",
    "    print('use coverages', cov_str)\n",
    "    \n",
    "    return saving_folder+'spec_training_covs_' + cov_str + '_p{}_' + xscale + limstr + '.png'\n",
    "\n",
    "    \n",
    "    \n",
    "if results_from_training and compute_stats:\n",
    "    different_cov_sets = [[0], [0,-1], list(range(len(class_coverage)))]\n",
    "    \n",
    "    for cov_i in different_cov_sets:\n",
    "\n",
    "        xscale='log'\n",
    "        filename= get_filename_spec_over_iter(cov_i, xscale, xlim=None)\n",
    "\n",
    "        draw_specialization_from_training(specialization_data,\n",
    "                                        'lenet',\n",
    "                                        hyperparams,\n",
    "                                        cov_i,\n",
    "                                        class_coverage,\n",
    "                                        xscale,\n",
    "                                        evaluation_scheme,\n",
    "                                        filename=filename,\n",
    "                                        save_figures=save_figures)\n",
    "\n",
    "        xscale='linear'\n",
    "        filename = get_filename_spec_over_iter(cov_i, xscale, xlim=None)\n",
    "        draw_specialization_from_training(specialization_data,\n",
    "                                        'lenet',\n",
    "                                        hyperparams,\n",
    "                                        cov_i,\n",
    "                                        class_coverage,\n",
    "                                        xscale,\n",
    "                                        evaluation_scheme,\n",
    "                                        filename=filename,\n",
    "                                        save_figures=save_figures)\n",
    "\n",
    "        xscale='linear'\n",
    "        xlim = (-1,100)\n",
    "        filename=get_filename_spec_over_iter(cov_i, xscale, xlim=xlim)\n",
    "            \n",
    "        draw_specialization_from_training(specialization_data,\n",
    "                                        'lenet',\n",
    "                                        hyperparams,\n",
    "                                        cov_i,\n",
    "                                        class_coverage,\n",
    "                                        xscale,\n",
    "                                        evaluation_scheme,\n",
    "                                        xlim=xlim,\n",
    "                                        filename=filename,\n",
    "                                        save_figures=save_figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "\n",
    "    tree_data = data['trees']\n",
    "    tree_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "\n",
    "    sparse_from = 'lenet'\n",
    "\n",
    "    metrics_tree = [\n",
    "        'tree depth',\n",
    "        'n leaves',\n",
    "        'average leaf depth', \n",
    "        f'useful neurons out of layer {arc_layers[sparse_from]+1} neurons'\n",
    "    ]\n",
    "    metrics_sc = [f'max support class {i}' for i in range(10)]\n",
    "    metrics_pc = [f'number of patterns for class {i}' for i in range(10)]\n",
    "    metrics_all = metrics_tree + metrics_sc + metrics_pc\n",
    "\n",
    "    metrics_dict = {\n",
    "        m: i for i, m in enumerate(metrics_all)\n",
    "\n",
    "    }\n",
    "\n",
    "    metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_densities_from_list_of_hyperparams(hyperp):\n",
    "    return [hyperp[i][1] for i in range(len(hyperp))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_filename_end(sparse_from, over_neurons, draw_init):\n",
    "    distr = 'with-init' if draw_init else 'without-init'\n",
    "    onstr = 'over-neurons' if over_neurons else 'over-density'\n",
    "    return f'{sparse_from}_{onstr}_{distr}.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "\n",
    "    for name in tree_data[list(tree_data.keys())[0]]:\n",
    "        def visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init):\n",
    "            filename = f'tree_data_{plot_name}_{get_tree_filename_end(name, over_neurons, draw_init)}'\n",
    "            draw_tree_data_constant_param(tree_data, metrics, name, over_neurons=over_neurons,\n",
    "                                  param_keys=param_keys,\n",
    "                                   lp_samples=lp_samples,\n",
    "                                  hyperparams_for_sparse=hyperparams,\n",
    "                                   arc_layers=arc_layers,\n",
    "                                   metrics_dict=metrics_dict,\n",
    "                                  draw_init=draw_init,\n",
    "                                  filename=saving_folder + filename,\n",
    "                                  save_figures=save_figures)\n",
    "\n",
    "        plot_name = ''\n",
    "        metrics = metrics_tree\n",
    "        over_neurons = False\n",
    "        draw_init = False\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n",
    "        over_neurons = True\n",
    "        draw_init = True\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n",
    "        plot_name = 'max-support'\n",
    "        metrics = metrics_sc\n",
    "        over_neurons = False\n",
    "        draw_init = False\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n",
    "        over_neurons = True\n",
    "        draw_init = True\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n",
    "        plot_name = 'unique_patterns'\n",
    "        metrics = metrics_pc\n",
    "        over_neurons = False\n",
    "        draw_init = False\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n",
    "        over_neurons = True\n",
    "        draw_init = True\n",
    "        visualize_tree_hyperp(plot_name, name, metrics, over_neurons, draw_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique activation patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    uaps = data['uaps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_stats and visualize_AP_based and not results_from_training:\n",
    "    for sparse_from in uaps[list(uaps.keys())[0]]:\n",
    "        draw_unique_aps(uaps, sparse_from,\n",
    "                       test_batch_size,\n",
    "                        hyperparams_for_sparse=hyperparams,\n",
    "                       blacklist=[],\n",
    "                       filename=saving_folder+f'unique-aps-{sparse_from}.png',\n",
    "                       xscale=xscale,\n",
    "                       save_figures=save_figures,\n",
    "                       auto_yscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique layer patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    ulps = data['ulps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_from = name\n",
    "\n",
    "if compute_stats and visualize_AP_based and not results_from_training:\n",
    "    for sparse_from in ulps[list(ulps.keys())[0]]:\n",
    "\n",
    "        draw_unique_lps(ulps, sparse_from, test_batch_size,\n",
    "                        hyperparams,\n",
    "                        blacklist=[],\n",
    "                       filename=saving_folder + f'unique_layer_patterns_{sparse_from}.png',\n",
    "                       xscale=xscale,\n",
    "                       save_figures=save_figures,\n",
    "                       auto_yscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    entropy_stats = data['entropy_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_stats and visualize_AP_based and not results_from_training:\n",
    "    for sparse_from in entropy_stats[list(entropy_stats.keys())[0]]:\n",
    "\n",
    "        draw_entropy(entropy_stats, sparse_from, test_batch_size, hyperparams,\n",
    "                         blacklist=[],\n",
    "                         filename=saving_folder + f'entropy-{sparse_from}.png',\n",
    "                         save_figures=save_figures,\n",
    "                         xscale=xscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of 2D ARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_data_2D = data['ar_2d'] if not results_from_training else data['ar_2d_training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    pre_aggregated = ar_data_2D['trained'][name][0].shape[-1] == 3\n",
    "    print('data is pre-aggregated ', pre_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_from_training:\n",
    "    draw_method = draw_number_of_2D_ARs_aggregated if pre_aggregated else draw_number_of_2D_ARs\n",
    "\n",
    "    def visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init):\n",
    "        wstr = 'wide' if wide_interval else 'narrow'\n",
    "        astr = 'normalized_by_area' if normalized_by_area else 'absolute_numbers'\n",
    "        arstr = 'just_area' if just_area else 'regions'\n",
    "        onstr = 'over_neurons' if over_neurons else 'over_densities'\n",
    "        istr = '_winit' if draw_init else ''\n",
    "\n",
    "        filename =  f'activation_regions_2D_{arstr}_{sparse_from}_{astr}_{wstr}_{onstr}_{istr}.png'\n",
    "        draw_method(ar_data_2D,\n",
    "                    sparse_from,\n",
    "                    normalized_by_area,\n",
    "                    average_over_images, \n",
    "                    hyperparams,\n",
    "                    param_keys=param_keys,\n",
    "                    over_neurons=over_neurons,\n",
    "                    n_models=n_models,\n",
    "                    wide_interval=wide_interval,\n",
    "                    just_area=just_area,\n",
    "                    draw_init=draw_init,\n",
    "                    use_three_samples=use_three_samples, # if we have different scenarios (planes through origin and spun by 3 imgs) set this to None\n",
    "                    filename=saving_folder + filename,\n",
    "                    save_figures=save_figures)\n",
    "\n",
    "    for sparse_from in ar_data_2D[list(ar_data_2D.keys())[0]]:\n",
    "            over_neurons=False\n",
    "            draw_init = False\n",
    "            just_area = False\n",
    "            wide_interval = False\n",
    "            normalized_by_area = True\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "\n",
    "            normalized_by_area = False\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "\n",
    "            over_neurons=True\n",
    "            normalized_by_area = True\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "            \n",
    "            normalized_by_area = False\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "            \n",
    "            draw_init = True\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "\n",
    "            draw_init = False\n",
    "            wide_interval = True\n",
    "            normalized_by_area = False\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "\n",
    "            just_area = True\n",
    "            wide_interval = True\n",
    "            visualize_2d_ars_hyperp(sparse_from, wide_interval, normalized_by_area, just_area, over_neurons, draw_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #AR by iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_by_iteration_filename(sparse_from, xlim, wide_interval, normalized_by_area, xscale):\n",
    "    wstr = 'wide' if wide_interval else 'narrow'\n",
    "    astr = 'normalized_by_area' if normalized_by_area else 'absolute_numbers'\n",
    "    limstr = 'xlim100' if xlim else ''\n",
    "\n",
    "    return saving_folder + f'ARs_by_iteration_2D_{sparse_from}_{astr}_{wstr}_' + 'p{}' + f'_{xscale}_{limstr}.png'\n",
    "\n",
    "if results_from_training and compute_stats:\n",
    "\n",
    "    sparse_from = 'lenet'\n",
    "    normalized_by_area = False\n",
    "    wide_interval = False\n",
    "    xscale = 'log'\n",
    "    \n",
    "    filename = get_ar_by_iteration_filename(sparse_from, False, wide_interval, normalized_by_area, xscale)\n",
    "    \n",
    "    draw_number_of_2D_ARs_by_iteration(ar_data_2D, \n",
    "                                       sparse_from, \n",
    "                                       normalized_by_area,\n",
    "                                       use_three_samples,\n",
    "                                      average_over_images, \n",
    "                                       evaluation_scheme,\n",
    "                                       hyperparams_for_sparse=hyperparams,\n",
    "                                       n_models=n_models,\n",
    "                                      wide_interval=wide_interval,\n",
    "                                       xscale=xscale,\n",
    "                                      filename=filename,\n",
    "                                      save_figures=save_figures)\n",
    "    \n",
    "    xscale = 'linear'\n",
    "    filename = get_ar_by_iteration_filename(sparse_from, False, wide_interval, normalized_by_area, xscale)\n",
    "    \n",
    "    draw_number_of_2D_ARs_by_iteration(ar_data_2D, \n",
    "                                       sparse_from, \n",
    "                                       normalized_by_area,\n",
    "                                       use_three_samples,\n",
    "                                      average_over_images, \n",
    "                                       evaluation_scheme,\n",
    "                                       hyperparams_for_sparse=hyperparams,\n",
    "                                       n_models=n_models,\n",
    "                                      wide_interval=wide_interval,\n",
    "                                       xscale=xscale,\n",
    "                                      filename=filename,\n",
    "                                      save_figures=save_figures)\n",
    "    \n",
    "    filename = get_ar_by_iteration_filename(sparse_from, True, wide_interval, normalized_by_area, xscale)\n",
    "    \n",
    "    draw_number_of_2D_ARs_by_iteration(ar_data_2D, \n",
    "                                       sparse_from, \n",
    "                                       normalized_by_area,\n",
    "                                       use_three_samples,\n",
    "                                      average_over_images, \n",
    "                                       evaluation_scheme,\n",
    "                                       hyperparams_for_sparse=hyperparams,\n",
    "                                       n_models=n_models,\n",
    "                                      wide_interval=wide_interval,\n",
    "                                       xscale=xscale,\n",
    "                                       xlim=(-1,100),\n",
    "                                      filename=filename,\n",
    "                                      save_figures=save_figures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
