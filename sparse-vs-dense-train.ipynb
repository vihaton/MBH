{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.data import get_train_and_test_loaders\n",
    "from scripts.models import *\n",
    "from scripts.visualizations import *\n",
    "from scripts.pruning import *\n",
    "from scripts.activation_patterns import *\n",
    "from scripts.stats import *\n",
    "from scripts.activation_regions import *\n",
    "from scripts.lp_decision_trees import *\n",
    "from scripts.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams\n",
    "...That you actually change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = 'sparse-vs-dense'\n",
    "test_name = 'example'\n",
    "\n",
    "hyperparams['misc'] = {\n",
    "    'notebook_name': notebook_name,\n",
    "    'test_name': test_name,\n",
    "    'random_seed': random_seed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models = True\n",
    "load_models = False # not implemented\n",
    "\n",
    "save_data = True\n",
    "compute_acc = True\n",
    "compute_stats = False # to compute specialization and AR-data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_models or load_models:\n",
    "    assert save_models is not load_models, 'you cannot load and save models at the same time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['experiments'] = {\n",
    "    'compute_stats': compute_stats,\n",
    "    'compute_acc': compute_acc,\n",
    "    'save_results': save_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data = False\n",
    "normalize_by_features = True # doesnt matter if normalize_data == False\n",
    "\n",
    "normalize_by_moving = True # doesnt matter if normalize_data == False\n",
    "normalize_by_scaling = True # doesnt matter if normalize_data == False\n",
    "\n",
    "batch_size = 60\n",
    "test_batch_size = 1000\n",
    "\n",
    "# digit, fashion\n",
    "data_name = 'fashion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dir = './mnist/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['data'] = {\n",
    "    'normalize_data': normalize_data,\n",
    "    'normalize_by_features': normalize_by_features,\n",
    "    'normalize_by_moving': normalize_by_moving,\n",
    "    'normalize_by_scaling': normalize_by_scaling,\n",
    "    'batch_size': batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'data_name': data_name,\n",
    "    'data_dir': training_data_dir\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hid_neurons = 400\n",
    "min_hid_neurons = 16\n",
    "\n",
    "n_sizes = 4\n",
    "size_scheme = 'log'\n",
    "\n",
    "smallest_net_densities = [1,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = get_sizes(n_sizes, max_hid_neurons, min_hid_neurons, size_scheme)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50000\n",
    "\n",
    "bias_std = 10**-6\n",
    "\n",
    "n_models = {'lenet': 5}\n",
    "n_models['deepfc'] = 0\n",
    "n_models['pipefc'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_total =  count_models_n_total(n_sizes, smallest_net_densities, n_models)\n",
    "print(f'models in total {models_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning hyperparams\n",
    "lr = 1.2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims_dict = get_hidden_dims(max_hid_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_scheme = get_eval_scheme(iterations)\n",
    "print(evaluation_scheme, len(evaluation_scheme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['models'] = {\n",
    "    'sizes': sizes,\n",
    "    'iterations': iterations,\n",
    "    'bias_std': bias_std,\n",
    "    'max_hid_neurons': max_hid_neurons,\n",
    "    'smallest_net_densities': smallest_net_densities,\n",
    "    'hidden_dims_dict': hidden_dims_dict,\n",
    "    'n_models': n_models,\n",
    "    'models_total': models_total,\n",
    "    'learning_rate': lr,\n",
    "    'evaluation_scheme': evaluation_scheme\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_weights = True # False -> prune nodes\n",
    "layer_wise_pruning = True # False -> global pruning (not implemented for pytorch pruning)\n",
    "\n",
    "prune_all_layers = False # should also the weights on the output layer be pruned?\n",
    "\n",
    "random_mask = True # does the random init network have a random mask as well?\n",
    "\n",
    "xscale = 'linear' # depends how we define the pruning rates, either linear or logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['pruning'] = {\n",
    "    'prune_weights': prune_weights,\n",
    "    'layer_wise_pruning': layer_wise_pruning,\n",
    "    'prune_all_layers': prune_all_layers,\n",
    "    'random_mask': random_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting ARs\n",
    "\n",
    "## 2D\n",
    "classes = [0,1,9] # shirts, pants, shoes\n",
    "classes = None # to randomize classes\n",
    "\n",
    "# how much one class samples should be covered?\n",
    "class_coverage = [0.8,0.95,0.99,1]\n",
    "\n",
    "use_three_samples = True\n",
    "average_over_images = 5\n",
    "\n",
    "# Mining LPs with Decision Trees\n",
    "lp_samples = 10000\n",
    "\n",
    "# Dark neurons\n",
    "dm_limit_perc = 0.01\n",
    "dark_mask_limit = int(dm_limit_perc*test_batch_size)\n",
    "print(f'dark mask flags neurons with n <= {dark_mask_limit} images (out of {test_batch_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['experiments']['classes'] = classes\n",
    "hyperparams['experiments']['class_coverage'] = class_coverage\n",
    "hyperparams['experiments']['use_three_samples'] = use_three_samples\n",
    "hyperparams['experiments']['average_over_images'] = average_over_images\n",
    "hyperparams['experiments']['lp_samples'] = lp_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architectures = []\n",
    "for name in n_models:\n",
    "    if n_models[name] > 0:\n",
    "        model_architectures.append(name)\n",
    "        \n",
    "print(model_architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_training = OrderedDict()\n",
    "ar_data_from_training = OrderedDict()\n",
    "spec_data_from_training = OrderedDict()\n",
    "\n",
    "data = OrderedDict()\n",
    "data['acc_training'] = data_from_training\n",
    "data['ar_2d_training'] = ar_data_from_training\n",
    "data['spec_training'] = spec_data_from_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_train_and_test_loaders(training_data_dir,\n",
    "                                                       data_name, batch_size,\n",
    "                                                       test_batch_size, normalize_data,\n",
    "                                                       normalize_by_features, normalize_by_moving,\n",
    "                                                       normalize_by_scaling, kwargs)\n",
    "\n",
    "train_samples = train_loader.dataset.data.shape[0]\n",
    "print(f'train data {train_loader.dataset.data.shape} on device {train_loader.dataset.data.device}')\n",
    "print(test_loader.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "input_features = train_loader.dataset.data.shape[1] * train_loader.dataset.data.shape[2]\n",
    "output_dim = len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = f'{data_name}-{ts}-{notebook_name}-{test_name}-{models_total}_models/'\n",
    "saving_folder = f'results/{folder_name}'\n",
    "if save_data and not os.path.isdir(saving_folder):\n",
    "    os.mkdir(saving_folder)\n",
    "    print('created folder for saving figures', saving_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(models_dir):\n",
    "    os.mkdir(models_dir)\n",
    "\n",
    "checkpoint_dir = models_dir + folder_name\n",
    "if save_models and not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "if save_models:\n",
    "    print(f'save models to {checkpoint_dir}')\n",
    "else:\n",
    "    print(\"don't save models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images that span the subspaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_stats:\n",
    "    n_example_sets, example_img_labels = get_n_spanning_image_groups(average_over_images, test_loader, classes)\n",
    "    for i, (example_imgs, labels) in enumerate(zip(n_example_sets, example_img_labels)):\n",
    "        visualize_example_images(example_imgs, labels,\n",
    "                                 classes_str=test_loader.dataset.classes,\n",
    "                                 horizontal=True\n",
    "                                )\n",
    "        if save_data:\n",
    "            plt.savefig(saving_folder+f'spanning_images_subspace_{i}.png')\n",
    "            \n",
    "    if save_data:\n",
    "        torch.save(example_img_labels, saving_folder+f'spanning_img_labels.pt')\n",
    "else:\n",
    "    print('no need for spanning images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folders = get_folders(models_dir)\n",
    "for i, folder in enumerate(model_folders):\n",
    "    print(i, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose, which models to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ind = 0\n",
    "if load_models and False: # adjust these variables depending which were used in the training of the models to be loaded\n",
    "    pruning_new_version = True\n",
    "    prune_weights = True\n",
    "    layer_wise_pruning = False\n",
    "\n",
    "if load_models:\n",
    "    path_to_models = models_dir + model_folders[dir_ind]\n",
    "    \n",
    "    sdict_files = get_pt_files_in_dir(path_to_models)\n",
    "    print(f'loading models from\\n\\t{model_folders[dir_ind]}')\n",
    "    if save_figures or save_data:\n",
    "        print(f'saving to\\n\\t{checkpoint_dir}')\n",
    "else:\n",
    "    print('dont load models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdicts = {}\n",
    "\n",
    "if load_models:\n",
    "    for file_name in sdict_files:\n",
    "        name, ending = file_name.split('.')\n",
    "        if ending != 'pt':\n",
    "            continue\n",
    "        sdicts[name] = torch.load(f'{path_to_models}/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the metadata to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_strings(prune_weights, layer_wise_pruning, bias_std, random_mask, prune_all_layers,\n",
    "                        normalize_data, normalize_by_features, normalize_by_moving, normalize_by_scaling,\n",
    "                        use_three_samples, lp_samples, class_coverage):\n",
    "    metadata = [\n",
    "        'loaded models' if load_models else 'fresh models',\n",
    "        'unstructured pruning' if prune_weights else 'structured pruning',\n",
    "        'pruning by layer' if layer_wise_pruning else 'pruning globally',\n",
    "        f'bias std {bias_std}',\n",
    "        f'random_mask' if random_mask else 'pruned_mask',\n",
    "        'prune all layers' if prune_all_layers else 'dont prune the output layer',\n",
    "        '2D plane is spun by origin and two images' if not use_three_samples else '2D plane is spun by three images',\n",
    "        f'use {lp_samples} to get layer patterns for specialization and dtree',\n",
    "        f'specialization blanket has {class_coverage}% sample coverage',\n",
    "        f'classes for the AR & specialization: {classes}'\n",
    "    ]\n",
    "    if normalize_data:\n",
    "        metadata += [\n",
    "            f'normalize data',\n",
    "            '\\tby features' if normalize_by_features else '\\tas whole',\n",
    "            '\\tmove data' if normalize_by_moving else '\\tdont move the data',\n",
    "            '\\tscale data' if normalize_by_scaling else '\\tdont scale the data'\n",
    "        ]\n",
    "    else:\n",
    "        metadata.append('dont normalize the data')\n",
    "\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata_strings(prune_weights, layer_wise_pruning, bias_std, random_mask, prune_all_layers,\n",
    "                        normalize_data, normalize_by_features, normalize_by_moving, normalize_by_scaling,\n",
    "                        use_three_samples, lp_samples, class_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metadata_to_file(fo):\n",
    "    newlinestr = '\\n'\n",
    "    fo.write(ts + newlinestr)\n",
    "    fo.write('dataset: ' + data_name + newlinestr)\n",
    "    fo.write('notebook: ' + notebook_name + newlinestr)\n",
    "    fo.write('test: ' + test_name + newlinestr)\n",
    "    for line in metadata:\n",
    "        fo.write(line + newlinestr)\n",
    "        \n",
    "if save_data:\n",
    "    with open(saving_folder + 'metadata.txt', 'w') as fo:\n",
    "        write_metadata_to_file(fo)\n",
    "    with open(saving_folder + 'metadata.json', 'w') as fo:\n",
    "        fo.write(json.dumps(hyperparams))\n",
    "    \n",
    "if save_models:\n",
    "    with open(checkpoint_dir + 'metadata.txt', 'w') as fo:\n",
    "        write_metadata_to_file(fo)\n",
    "    with open(checkpoint_dir + 'metadata.json', 'w') as fo:\n",
    "        fo.write(json.dumps(hyperparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple dense network - Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same architecture as Zhou et al. (https://arxiv.org/pdf/1905.01067.pdf) and Frankle & Carbin (https://arxiv.org/pdf/1803.03635.pdf), ie. Lenet from LeCun et al. 1998.\n",
    "\n",
    "| FC layers | 300, 100, 10 |\n",
    "|:--|:--|\n",
    "| Weights | 266k  |\n",
    "| Iterations | 50k |\n",
    "| Batch size | 60 |\n",
    "| initialization | normal distribution std=0.1 |\n",
    "| Optimizer | Adam 1.2e-3 |\n",
    "| Pruning Rate (for iterative) | fc 20% |\n",
    "| Loss Function | Cross Entropy Loss |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slightly deeper dense network\n",
    "\n",
    "| FC layers | 200, 100, 100, 10 |\n",
    "|:--|:--|\n",
    "| Weights | 188k  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipefc (layers have shared width)\n",
    "\n",
    "| FC layers | 100, 100, 100, 10 |\n",
    "|:--|:--|\n",
    "| Parameters | 100k  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the networks sparse networks with the same #params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_dims = get_architecture_dims(hidden_dims_dict, sizes, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_params = get_dense_params(arc_dims, input_features, smallest_net_densities)\n",
    "dense_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_models:\n",
    "    hyperparams_for_sparse = get_hyperparams_for_sparses(dense_params,\n",
    "                                                         arc_dims,\n",
    "                                                         n_models,\n",
    "                                                         smallest_net_densities=smallest_net_densities,\n",
    "                                                         count_output=prune_all_layers,\n",
    "                                                         input_dim=input_features)\n",
    "else:\n",
    "    hyperparams_for_sparse = sdicts['hyperparams']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_hyperparams(hyperparams_for_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_keys = OrderedDict()\n",
    "for name in hyperparams_for_sparse:\n",
    "    param_keys[name] = [\n",
    "        hyp[0][-1] for hyp in hyperparams_for_sparse[name]\n",
    "    ]\n",
    "    \n",
    "param_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_hyperparam_setup(hyperparams_for_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_models:\n",
    "    models = init_models_sparse_vs_dense(hyperparams_for_sparse, n_models, input_features, output_dim, bias_std,\n",
    "                                        random_mask, prune_all_layers, prune_weights)\n",
    "else:\n",
    "    models = sdicts['models_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update n_models to be accurate\n",
    "if load_models:\n",
    "    n_models = {}\n",
    "    for name in models.keys():\n",
    "        for params in models[name].keys():\n",
    "            n_models[name] = len(models[name][params][0])\n",
    "            break\n",
    "    print(f'n_models: {n_models}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_information(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_models:\n",
    "    torch.save(hyperparams_for_sparse, checkpoint_dir + 'hyperparams.pt')\n",
    "    torch.save(models, checkpoint_dir + 'models_init.pt')\n",
    "    print('models saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(train_log, val_log):\n",
    "    print(f'\\ttraining \\tloss {train_log[-1][0]:.3f} acc {train_log[-1][1]:.2f}%')\n",
    "    print(f'\\tvalidation \\tloss {val_log[-1][0]:.3f} acc {val_log[-1][1]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_in_middle_of_training(model, test_loader, lp_samples, coverage, example_image_sets, \n",
    "                                        average_over_images, use_three_samples=True, n_classes=10):\n",
    "    '''return ar data, specialization data'''\n",
    "    with torch.no_grad():\n",
    "        _, _, max_patterns = record_lps_and_max_patterns(model,\n",
    "                                                         test_loader,\n",
    "                                                         n_samples=lp_samples,\n",
    "                                                         n_classes=n_classes,\n",
    "                                                         coverage=coverage)\n",
    "        ars, specs = compute_2D_ARs_and_specialization_for_subspaces(model,\n",
    "                                                                    example_image_sets=example_image_sets,\n",
    "                                                                    average_over_images=average_over_images,\n",
    "                                                                    use_three_samples=True,\n",
    "                                                                     max_patterns=max_patterns\n",
    "                                                                  )\n",
    "    return ars, specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterations, lr, evaluation_scheme, average_over_images=5, print_every_n_iteration=100, compute_stats=False, compute_acc=True, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    if type(evaluation_scheme) is int:\n",
    "        evaluation_scheme = list(range(evaluation_scheme, iterations / evaluation_scheme, evaluation_scheme))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    #before any training\n",
    "    val_log, train_log, ar_data, spec_data = [], [], [], []\n",
    "    i = 0\n",
    "    \n",
    "    if compute_acc:\n",
    "        val_log.append(evaluate(model, test_loader, loss_fn))\n",
    "        train_log.append(evaluate(model, train_loader, loss_fn))\n",
    "        \n",
    "    if verbose:\n",
    "        plur = 's' if i > 1 else ''\n",
    "        print(f' after {i} iteration{plur}')\n",
    "        print_progress(train_log, val_log)\n",
    "\n",
    "    if compute_stats:\n",
    "        ars, specs = compute_stats_in_middle_of_training(model, test_loader, lp_samples, class_coverage, n_example_sets, average_over_images)\n",
    "        ar_data.append(ars)\n",
    "        spec_data.append(specs)\n",
    "    \n",
    "    while i < iterations:\n",
    "\n",
    "        total_loss = 0\n",
    "        correct_pred_n = 0\n",
    "        for data, target in train_loader:\n",
    "            if i >= iterations:\n",
    "                break\n",
    "                \n",
    "            if verbose and i % print_every_n_iteration == 0:\n",
    "                print(f'iteration {i}/{iterations}', end='\\r')\n",
    "\n",
    "            data = data.view(-1,28*28) # flatten\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            i += 1 # we have trained one more iteration\n",
    "            \n",
    "            if i in evaluation_scheme and (compute_stats or compute_acc):\n",
    "                with torch.no_grad():            \n",
    "\n",
    "                    if compute_acc:\n",
    "                        train_log.append(evaluate(model, train_loader, loss_fn))\n",
    "                        val_log.append(evaluate(model, test_loader, loss_fn))\n",
    "                        \n",
    "                    if verbose:\n",
    "                        plur = 's' if i > 1 else ''\n",
    "                        print(f' after {i} iteration{plur}')\n",
    "                        print_progress(train_log, val_log)\n",
    "                    \n",
    "                    if compute_stats:\n",
    "                        ars, specs = compute_stats_in_middle_of_training(model, test_loader, lp_samples, class_coverage, n_example_sets, average_over_images)\n",
    "                        ar_data.append(ars)\n",
    "                        spec_data.append(specs)\n",
    "\n",
    "                    total_loss = 0\n",
    "                    correct_pred_n = 0\n",
    "                \n",
    "\n",
    "    return np.dstack((train_log, val_log)), np.array(ar_data), np.array(spec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "verbose = True\n",
    "k = 0\n",
    "if not load_models:\n",
    "    for name in models:\n",
    "        print(name)\n",
    "        models_p = models[name]\n",
    "        td, regions, specializations = OrderedDict(), OrderedDict(), OrderedDict()\n",
    "\n",
    "        for i, params in enumerate(list(models_p.keys())):\n",
    "            models_h = models_p[params]\n",
    "            td_h, regions_h, specs_h = [], [], []\n",
    "            for j, models_n in enumerate(models_h):\n",
    "                td_n, regions_n, specs_n = [], [], []\n",
    "                for model in models_n:\n",
    "                    k += 1\n",
    "                    end = '\\n' if verbose else '\\r'\n",
    "                    print(f'train model {k}/{models_total}', end=end)\n",
    "                    t, regs, specs = train(model, iterations, lr, evaluation_scheme,\n",
    "                                    average_over_images=average_over_images,\n",
    "                                    compute_stats=compute_stats,\n",
    "                                    compute_acc=compute_acc,\n",
    "                                    verbose=verbose)\n",
    "                    td_n.append(t)\n",
    "                    regions_n.append(regs)\n",
    "                    specs_n.append(specs)\n",
    "                td_h.append(td_n)\n",
    "                regions_h.append(regions_n)\n",
    "                specs_h.append(specs_n)\n",
    "                \n",
    "            td[params] = np.array(td_h)\n",
    "            regions[params] = np.array(regions_h)\n",
    "            specializations[params] = np.array(specs_h)\n",
    "            \n",
    "        data_from_training[name] = td\n",
    "        ar_data_from_training[name] = regions\n",
    "        spec_data_from_training[name] = specializations\n",
    "        \n",
    "        models_trained = models\n",
    "else:\n",
    "    print('Models will be loaded, no training needed.')\n",
    "    models = sdicts['models_trained']\n",
    "    model_architectures = list(models.keys())\n",
    "    \n",
    "    \n",
    "# save\n",
    "\n",
    "if save_models:\n",
    "    torch.save(models, checkpoint_dir + 'models_trained.pt')\n",
    "    print('models saved')\n",
    "else:\n",
    "    print('dont save models')\n",
    "    \n",
    "if save_data and (compute_stats or compute_acc):\n",
    "    torch.save(data, saving_folder + 'data.pt')\n",
    "    torch.save(hyperparams_for_sparse, saving_folder + 'hyperparams.pt')\n",
    "    print('saved')\n",
    "elif not save_data:\n",
    "    print('dont save data')\n",
    "else:\n",
    "    print('nothing to save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained dense networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_models:\n",
    "    torch.save(models, checkpoint_dir + 'models_trained.pt')\n",
    "    print('models saved')\n",
    "else:\n",
    "    print('dont save models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data and (compute_stats or compute_acc):\n",
    "    torch.save(data, saving_folder + 'data.pt')\n",
    "    torch.save(hyperparams_for_sparse, saving_folder + 'hyperparams.pt')\n",
    "    print('saved')\n",
    "elif not save_data:\n",
    "    print('dont save data')\n",
    "else:\n",
    "    print('nothing to save')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
